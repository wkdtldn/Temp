{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from Tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "# 수행 경로 설정\n",
    "os.chdir(\"/Users/jangsiu/workspace/DiaryProject/backend/data\")\n",
    "\n",
    "# test,train data 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "\n",
    "# train,test data 정의\n",
    "train_data = pd.read_table(\"ratings_train.txt\")\n",
    "test_data = pd.read_table(\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train, test data 중복삭제, 널값 없애기, 특수문자 제거**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 요소 삭제\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'],inplace=True)\n",
    "\n",
    "# # Null 값 제거\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "# 한글, 공백 제외, 모두 \n",
    "train_data['document'] = train_data['document'].str.replace(r\"[^가-힣 ]\", '',regex=True)\n",
    "test_data['document'] = test_data['document'].str.replace(r\"[^가-힣 ]\", '', regex=True)\n",
    "\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 특수 기호 등 제거 후, Null 값 제거\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "test_data = test_data.dropna(how = 'any')\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145663/145663 [04:35<00:00, 528.01it/s]\n",
      "100%|██████████| 48929/48929 [01:36<00:00, 506.84it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    train_tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    train_stopwords_removed_sentence = [word for word in train_tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(train_stopwords_removed_sentence)\n",
    "\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    test_tokenized_sentence = okt.morphs(sentence, stem=True) # 토근화\n",
    "    test_stopwords_removed_sentence = [word for word in test_tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(test_stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mfit_morpheme(X_train)\n\u001b[1;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mfit_morpheme(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train[:\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/DiaryProject/backend/Tokenizer.py:27\u001b[0m, in \u001b[0;36mfit_morpheme\u001b[0;34m(self, morph_list)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_word_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# encoding_list 초기 구현\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(morph_list):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_list\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "X_train = tokenizer.fit_morpheme(X_train)\n",
    "X_test = tokenizer.fit_morpheme(X_test)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train = tokenizer.by_index(X_train)\n",
    "# X_test = tokenizer.by_index(X_test)\n",
    "\n",
    "# threshold = 3\n",
    "# total_cnt = len(X_train)\n",
    "# rare_cnt = 0\n",
    "# total_freq = 0\n",
    "# rare_freq = 0\n",
    "\n",
    "# for key, value in X_train.items():\n",
    "#     total_freq += value\n",
    "\n",
    "#     if (value < threshold):\n",
    "#         rare_cnt += 1\n",
    "#         rare_freq += value\n",
    "\n",
    "# print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "# print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "# print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "# print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
